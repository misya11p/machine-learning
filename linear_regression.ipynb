{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 線形回帰\n",
    "\n",
    "最も簡単な機械学習モデルを学んでいこう。回帰モデルを作る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「最も簡単な関数」と言われて何を思い浮かべるだろうか。一次関数を思い浮かべる人は多いのではないだろうか。おそらく中学校で初めに習うだろう。\n",
    "\n",
    "$$\n",
    "f(x) = ax + b\n",
    "$$\n",
    "\n",
    "線形回帰ではこの一次関数がモデルとなる。  \n",
    "一次関数はグラフで表すと直線となる。数学では「線形」は直線を意味する。線形回帰という名前はここから納得できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 最小二乗法\n",
    "\n",
    "予測値と正解の差の二乗和が最小になるようなパラメータを求める手法。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x) &= ax + b \\\\\n",
    "E\n",
    "    &= \\sum_{i=1}^{n} (y_i - f(x_i))^2 \\\\\n",
    "    &= \\sum_{i=1}^{n} (y_i - ax_i - b)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "定義について、適当にググった感じ、線形回帰で上記を行う場合に限定している記述と、そうでない記述を両方見かける。「最小二乗法」という文字だけ見れば特に線形回帰に限定する必要もなさそうだけど、線形回帰以外で（モデルに依らず二乗和誤差を最小にするという文脈で）この言葉が使われている場面をあまり見ないので、線形回帰に限定して認識してもいいかも。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "線形回帰の学習では基本的に最小二乗法が使われる。最小二乗法を使うことでパラメータを解析的に解くことができる。損失関数として二乗和誤差を使うことで目的関数がパラメータに関する二次関数になるので、微分によって最適解を求めることができる。差の絶対値ではなく二乗を使う意味がここにある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最尤推定との関係\n",
    "\n",
    "最小二乗法は誤差に正規分布を仮定した最尤推定と見ることができる。差の二乗を取る意味はここからも納得できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの予測値と正解の差が正規分布に従うと仮定して、確率モデル$p(x)$を以下のように定義する。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x)\n",
    "    &= \\mathcal N(x; f(x), \\sigma^2) \\\\\n",
    "    &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(x_i - f(x))^2}{2\\sigma^2} \\right) \\\\\n",
    "    &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(y_i - ax - b)^2}{2\\sigma^2} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "このとき、対数尤度$\\log p(x)$を最大化するパラメータ$a,b$を考える。  \n",
    "上記の対数尤度を変形すると\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(x)\n",
    "    &= \\log \\prod_{i=1}^{n} p(x_i)\\\\\n",
    "    &= \\sum_{i=1}^{n} \\log p(x_i) \\\\\n",
    "    &= \\sum_{i=1}^{n} \\log \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(y_i - ax_i - b)^2}{2\\sigma^2} \\right)\\right) \\\\\n",
    "    &= \\sum_{i=1}^{n} \\left( \\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}} -\\frac{(y_i - ax_i - b)^2}{2\\sigma^2} \\right) \\\\\n",
    "    &= -\\frac{n}{2} \\log 2\\pi\\sigma^2 - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - ax_i - b)^2 \\\\\n",
    "    &= -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - ax_i - b)^2 + \\text{const}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "となる。この最大化は$\\sum_{i=1}^{n} (y_i - ax_i - b)^2$の最小化と同じになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
